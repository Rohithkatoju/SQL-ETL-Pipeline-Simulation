create database pipeline;

use pipeline;

-- 1. Introduction
-- The aim of this project is to simulate an ETL (Extract, Transform, Load) process using Oracle SQL and DB Browser.
-- ETL pipelines are essential in data warehousing and business intelligence for processing raw data into meaningful information.
-- This project demonstrates each stage of ETL: importing raw data, cleaning it, transforming it, and loading it into production-ready tables, with logging and automation using triggers.

-- 2. Abstract
-- This project simulates a simple data pipeline that processes a raw CSV dataset containing customer information.
-- The process begins with importing the data into a staging table, followed by data cleansing (removing nulls and duplicates), transformation,
-- and final loading into a production table. An audit table tracks the number of records inserted during each load.
-- A trigger is implemented to automatically clean up staging data after successful inserts. Finally, the clean tables and logs are exported using DB Browser.

-- 3. Tools Used
-- Oracle SQL: For database operations, scripting, and triggers
-- DB Browser for SQLite: For CSV editing, testing, and exporting final tables
-- CSV Dataset: Manually created or publicly sourced dataset for simulation (e.g., customers.csv)

-- 4. Steps Involved in Building the project
-- Step 1 : Import Raw Data
-- A sample CSV file (customers.csv) is imported into a staging table using DB Browser or Oracle SQL tools.

create table staging_customers (
	customer_id varchar(10),
    full_name varchar(100),
    email varchar(100),
    signup_date date );
    
-- Step 2: Clean Data
-- Remove NULL entries and duplicates based on customer_id.

delete from staging_customers
where customer_id is null or full_name is null;

delete from staging_customers s
where rowid > (
	select min(rowid)
    from staging_customers
    where s.customer_id = customer_id
    );

-- Step 3: Transform & Load
-- Insert cleaned data into a production table.

create table prod_customers (
	customer_id varchar(10) primary key,
    full_name varchar(100),
    email varchar(100),
    signup_date date );

insert into prod_customers
select * from staging_customers;

-- Step 4: Audit Logging
-- Create an audit table and record the number of inserted rows.

create table etl_audit (
	audit_id number generated by default as identity,
    table_name varchar(50),
    inserted_at timestamp default systimestamp,
    records_inserted bigint );
    
insert into etl_audit (table_name, records_inserted)
select 'prod_customers', count(*) from staging_customers;

-- Step 5: Automate Cleanup
-- Trigger to delete inserted records from staging.

create or replace trigger  trg_cleanup_staging
after insert on prod_customers
begin 
	delete from staging_customers
    where customer_id in ( select customer_id from prod_customers);
end;

-- Step 6 : Export Final Data
-- Use DB Browser or Oracle SQL Developer to export:
-- -- Cleaned prod_customers table
-- -- ETL log from etl_audit

-- 5. Conclusion
-- This project provides practical experience in building an SQL-based ETL pipeline using Oracle SQL.
-- From ingesting raw data to automating the cleanup process using triggers, the simulation covers essential operations used in data engineering and analytics projects.
-- The resulting production tables and audit logs are clean, well-documented, and export-ready.
